
Risk measures are statistical measures that are historical predictors of investment risk and volatility, and they are also major components in modern portfolio theory (MPT). MPT is a standard financial and academic methodology for assessing the performance of a stock or a stock fund as compared to its benchmark index.
 
There are five principal risk measures, and each measure provides a unique way to assess the risk present in investments that are under consideration. The five measures include the alpha, beta, R-squared, standard deviation, and Sharpe ratio. Risk measures can be used individually or together to perform a risk assessment. When comparing two potential investments, it is wise to compare like for like to determine which investment holds the most risk.
 
Alpha measures risk relative to the market or a selected benchmark index. For example, if the S&P 500 has been deemed the benchmark for a particular fund, the activity of the fund would be compared to that experienced by the selected index. If the fund outperforms the benchmark, it is said to have a positive alpha. If the fund falls below the performance of the benchmark, it is considered to have a negative alpha.
 
Beta measures the volatility or systemic risk of a fund in comparison to the market or the selected benchmark index. A beta of one indicates the fund is expected to move in conjunction with the benchmark. Betas below one are considered less volatile than the benchmark, while those over one are considered more volatile than the benchmark.
 
R-Squared measures the percentage of an investment's movement attributable to movements in its benchmark index. An R-squared value represents the correlation between the examined investment and its associated benchmark. For example, an R-squared value of 95 would be considered to have a high correlation, while an R-squared value of 50 may be considered low.
 
The U.S. Treasury Bill functions as a benchmark for fixed-income securities, while the S&P 500 Index functions as a benchmark for equities.
 
Standard deviation is a method of measuring data dispersion in regards to the mean value of the dataset and provides a measurement regarding an investment’s volatility.
 
As it relates to investments, the standard deviation measures how much return on investment is deviating from the expected normal or average returns.
 
The Sharpe ratio measures performance as adjusted by the associated risks. This is done by removing the rate of return on a risk-free investment, such as a U.S. Treasury Bond, from the experienced rate of return.
 
This is then divided by the associated investment’s standard deviation and serves as an indicator of whether an investment's return is due to wise investing or due to the assumption of excess risk.
