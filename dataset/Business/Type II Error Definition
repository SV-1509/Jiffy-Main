
A type II error is a statistical term used within the context of hypothesis testing that describes the error that occurs when one accepts a null hypothesis that is actually false. A type II error produces a false negative, also known as an error of omission. For example, a test for a disease may report a negative result, when the patient is, in fact, infected. This is a type II error because we accept the conclusion of the test as negative, even though it is incorrect.
 
In statistical analysis, a type I error is the rejection of a true null hypothesis, whereas a type II error describes the error that occurs when one fails to reject a null hypothesis that is actually false. The error rejects the alternative hypothesis, even though it does not occur due to chance.
 
A type II error, also known as an error of the second kind or a beta error, confirms an idea that should have been rejected, such as, for instance, claiming that two observances are the same, despite them being different. A type II error does not reject the null hypothesis, even though the alternative hypothesis is the true state of nature. In other words, a false finding is accepted as true.
 
A type II error can be reduced by making more stringent criteria for rejecting a null hypothesis. For example, if an analyst is considering anything that falls within the +/- bounds of a 95% confidence interval as statistically insignificant (a negative result), then by decreasing that tolerance to +/- 90%, and subsequently narrowing the bounds, you will get fewer negative results, and thus reduce the chances of a false negative.
 
Taking these steps, however, tends to increase the chances of encountering a type I error—a false positive result. When conducting a hypothesis test, the probability or risk of making a type I error or type II error should be considered.
 The steps taken to reduce the chances of encountering a type II error tend to increase the probability of a type I error. 
The difference between a type II error and a type I error is that a type I error rejects the null hypothesis when it is true (i.e. a false positive). The probability of committing a type I error is equal to the level of significance that was set for the hypothesis test. Therefore, if the level of significance is 0.05, there is a 5% chance a type I error may occur.
 
The probability of committing a type II error is equal to one minus the power of the test, also known as beta. The power of the test could be increased by increasing the sample size, which decreases the risk of committing a type II error.
 
Assume a biotechnology company wants to compare how effective two of its drugs are for treating diabetes. The null hypothesis states the two medications are equally effective. A null hypothesis, H0, is the claim that the company hopes to reject using the one-tailed test. The alternative hypothesis, Ha, states the two drugs are not equally effective. The alternative hypothesis, Ha, is the state of nature that is supported by rejecting the null hypothesis.
 
The biotech company implements a large clinical trial of 3,000 patients with diabetes to compare the treatments. The company randomly divides the 3,000 patients into two equally sized groups, giving one group one of the treatments and the other group the other treatment. It selects a significance level of 0.05, which indicates it is willing to accept a 5% chance it may reject the null hypothesis when it is true or a 5% chance of committing a type I error.
 
Assume the beta is calculated to be 0.025, or 2.5%. Therefore, the probability of committing a type II error is 2.5%. If the two medications are not equal, the null hypothesis should be rejected. However, if the biotech company does not reject the null hypothesis when the drugs are not equally effective, a type II error occurs.
